<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Lewis Crouch" />


<title>Sentiment Analysis via R, FeatureHashing and XGBoost</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
margin: 0 auto;
background-color: white;

/ font-family:Georgia, Palatino, serif;
font-family: "Open Sans", "Book Antiqua", Palatino, serif;
/ font-family:Arial, Helvetica, sans-serif;
/ font-family:Tahoma, Verdana, Geneva, sans-serif;
/ font-family:Courier, monospace;
/ font-family:"Times New Roman", Times, serif;
	color: #333333; 
/ color: #000000; 
/ color: #666666; 	/ color: #E3E3E3; 
/ color: white; line-height: 100%;
max-width: 800px;
padding: 10px;
font-size: 17px;
text-align: justify;
text-justify: inter-word;
}
p {
line-height: 150%;
/ max-width: 540px;
max-width: 960px;
margin-bottom: 5px;
font-weight: 400; / color: #333333
}
h1, h2, h3, h4, h5, h6 {
font-weight: 400;
margin-top: 35px;
margin-bottom: 15px;
padding-top: 10px;
}
h1 {
margin-top: 70px;
color: #606AAA;
font-size:230%;
font-variant:small-caps;
padding-bottom:20px;
width:100%;
border-bottom:1px solid #606AAA;
}
h2 {
font-size:160%;
}
h3 {
font-size:130%;
}
h4 {
font-size:120%;
font-variant:small-caps;
}
h5 {
font-size:120%;
}
h6 {
font-size:120%;
font-variant:small-caps;
}
a {
color: #606AAA;
margin: 0;
padding: 0;
vertical-align: baseline;
}
a:hover {
text-decoration: blink;
color: green;
}
a:visited {
color: gray;
}
ul, ol {
padding: 0;
margin: 0px 0px 0px 50px;
}
ul {
list-style-type: square;
list-style-position: inside;
}
li {
line-height:150% }
li ul, li ul {
margin-left: 24px;
}
pre {
padding: 0px 10px;
max-width: 800px;
white-space: pre-wrap;
}
code {
font-family: Consolas, Monaco, Andale Mono, monospace, courrier new;
line-height: 1.5;
font-size: 15px;
background: #F8F8F8;
border-radius: 4px;
padding: 5px;
display: inline-block;
max-width: 800px;
white-space: pre-wrap;
}
li code, p code {
background: #CDCDCD;
color: #606AAA;
padding: 0px 5px 0px 5px;
}
code.r, code.cpp {
display: block;
word-wrap: break-word;
border: 1px solid #606AAA; }
aside {
display: block;
float: right;
width: 390px;
}
blockquote {
border-left:.5em solid #606AAA;
background: #F8F8F8;
padding: 0em 1em 0em 1em;
margin-left:10px;
max-width: 500px;
}
blockquote cite {
line-height:10px;
color:#bfbfbf;
}
blockquote cite:before {
/content: '\2014 \00A0';
}
blockquote p, blockquote li { color: #666;
}
hr {
/ width: 540px;
text-align: left;
margin: 0 auto 0 0;
color: #999;
}

table {
width: 100%;
border-top: 1px solid #919699;
border-left: 1px solid #919699;
border-spacing: 0;
}
table th {
padding: 4px 8px 4px 8px;
text-align: center;
color: white;
background: #606AAA;
border-bottom: 1px solid #919699;
border-right: 1px solid #919699;
}
table th p {
font-weight: bold;
margin-bottom: 0px; }
table td {
padding: 8px;	vertical-align: top;
border-bottom: 1px solid #919699;
border-right: 1px solid #919699;
}
table td:last-child {
/background: lightgray;
text-align: right;
}
table td p {
margin-bottom: 0px; }
table td p + p {
margin-top: 5px; }
table td p + p + p {
margin-top: 5px; }</style>




</head>

<body>




<h1 class="title toc-ignore">Sentiment Analysis via R, FeatureHashing
and XGBoost</h1>
<h4 class="author">Lewis Crouch</h4>


<div id="TOC">
<ul>
<li><a href="#why-featurehashing" id="toc-why-featurehashing"><span class="toc-section-number">1</span> Why FeatureHashing?</a></li>
<li><a href="#package-versions" id="toc-package-versions"><span class="toc-section-number">2</span> Package versions</a></li>
<li><a href="#basic-data-preparation" id="toc-basic-data-preparation"><span class="toc-section-number">3</span> Basic data preparation</a></li>
<li><a href="#featurehashing" id="toc-featurehashing"><span class="toc-section-number">4</span> FeatureHashing</a></li>
<li><a href="#training-xgboost" id="toc-training-xgboost"><span class="toc-section-number">5</span> Training XGBoost</a></li>
</ul>
</div>

<p>This vignette demonstrates a sentiment analysis task, using the <a href="https://github.com/wush978/FeatureHashing">FeatureHashing
package</a> for data preparation (instead of more established text
processing packages such as ‘tm’) and the <a href="https://github.com/dmlc/xgboost">XGBoost package</a> to train a
classifier (instead of packages such as glmnet).</p>
<p>With thanks to Maas et al (2011) <em>Learning Word Vectors for
Sentiment Analysis</em> we make use of the ‘Large Movie Review Dataset’.
In their own words:</p>
<blockquote>
<p>We constructed a collection of 50,000 reviews from IMDB, allowing no
more than 30 reviews per movie. The constructed dataset contains an even
number of positive and negative reviews, so randomly guessing yields 50%
accuracy. Following previous work on polarity classification, we
consider only highly polarized reviews. A negative review has a score
less than or equal to 4 out of 10, and a positive review has a score
equal to or greater than 7 out of 10. Neutral reviews are not included
in the dataset. In the interest of providing a benchmark for future work
in this area, we release this dataset to the public.</p>
</blockquote>
<p>This data is also available via <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial">Kaggle</a> and the
format provided there makes for a more convenient starting point, simply
because all of the training data is in a single file. In our case we
will only use the training data - 25,000 reviews - as we’ll only go as
far as checking our classifier via validation.</p>
<div id="why-featurehashing" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Why
FeatureHashing?</h1>
<p>It’s not essential to use FeatureHashing for this movie review
dataset - a combination of the tm and glmnet packages works reasonably
well here - but it’s a convenient way to illustrate the benefits of
FeatureHashing. For example, we will see how easily we can select the
size of the hashed representations of the review texts and will
understand the options FeatureHashing makes available for processing the
data in subsets.</p>
<p>The combination of FeatureHashing and XGBoost can also be seen as a
way to access some of the benefits of the Vowpal Wabbit approach to
machine learning, without switching to a fully online learner. By using
the ‘hashing trick’, FeatureHashing easily handles features of many
possible categorical values. These are then stored in a sparse,
low-memory format on which XGBoost can quickly train a linear classifier
using a gradient descent approach. At a minimum this is a useful way to
better understand how tools like Vowpal Wabbit push the same approaches
to their limits. But in our case we get to benefit from these approaches
without leaving the R environment.</p>
</div>
<div id="package-versions" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Package versions</h1>
<p>This vignette uses FeatureHashing v9.0 and XGBoost v0.3-3.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(FeatureHashing); <span class="fu">library</span>(Matrix); <span class="fu">library</span>(xgboost)</span></code></pre></div>
</div>
<div id="basic-data-preparation" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Basic data
preparation</h1>
<p>First we read the training data and perform some simple text cleaning
using gsub() to remove punctuation before converting the text to
lowercase. At this stage each review is read and stored as a single
continuous string.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>imdb <span class="ot">&lt;-</span> <span class="fu">read.delim</span>(<span class="st">&quot;Data/labeledTrainData.tsv&quot;</span>, <span class="at">quote =</span> <span class="st">&quot;&quot;</span>, <span class="at">as.is =</span> T)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>imdb<span class="sc">$</span>review <span class="ot">&lt;-</span> <span class="fu">tolower</span>(<span class="fu">gsub</span>(<span class="st">&quot;[^[:alnum:] ]&quot;</span>, <span class="st">&quot; &quot;</span>, imdb<span class="sc">$</span>review))</span></code></pre></div>
<p>Which, using one of the shortest reviews as an example, leaves us
with the following. At this stage, the review is still being stored as
single character string and we only use strwrap() for pretty
printing:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">strwrap</span>(imdb<span class="sc">$</span>review[<span class="dv">457</span>], <span class="at">width =</span> <span class="dv">80</span>)</span></code></pre></div>
<pre><code>## [1] &quot;kurosawa is a proved humanitarian this movie is totally about people living in&quot; 
## [2] &quot;poverty you will see nothing but angry in this movie it makes you feel bad but&quot; 
## [3] &quot;still worth all those who s too comfortable with materialization should spend 2&quot;
## [4] &quot;5 hours with this movie&quot;</code></pre>
</div>
<div id="featurehashing" class="section level1" number="4">
<h1><span class="header-section-number">4</span> FeatureHashing</h1>
<p>We can then hash each of our review texts into a document term
matrix. We’ll choose the simpler binary matrix representation rather
than term frequency. The FeatureHashing package provides a convenient
split() function to split each review into words, before then hashing
each of those words/terms to an integer value to use as a column
reference in a sparse matrix.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">hashed.model.matrix</span>(<span class="sc">~</span> <span class="fu">split</span>(review, <span class="at">delim =</span> <span class="st">&quot; &quot;</span>, <span class="at">type =</span> <span class="st">&quot;tf-idf&quot;</span>),</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>                          <span class="at">data =</span> imdb, <span class="at">hash.size =</span> <span class="dv">2</span><span class="sc">^</span><span class="dv">16</span>, <span class="at">signed.hash =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The other important choice we’ve made is the hash.size of 2^16. This
limits the number of columns in the document term matrix and is how we
convert a feature of an unknown number of categories to a binary
representation of known, fixed size. For the sake of speed and to keep
memory requirements to a minimum, we’re using a relatively small value
compared to the number of unique words in this dataset. This parameter
can be seen as a hyperparameter to be tuned via validation.</p>
<p>The resulting 50MB dgCMatrix is the sparse format used by the Matrix
package that ships with base R. A dense representation of the same data
would occupy 12GB. Just out of curiosity, we can readily check the new
form of our single review example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">as.integer</span>(<span class="fu">which</span>(d1[<span class="dv">457</span>, ] <span class="sc">!=</span> <span class="dv">0</span>))</span></code></pre></div>
<pre><code>##  [1]     1  2780  6663 12570 13886 16269 18258 19164 19665 20531 22371
## [12] 22489 26981 28697 29324 32554 33091 33251 35321 35778 35961 37510
## [23] 38786 39382 45651 46446 51516 52439 54827 57399 57784 58791 59061
## [34] 60097 61317 62283 62878 62906 62941 63295</code></pre>
<p>The above transformation is independent of the other reviews and as
long as we use the same options in hashed.model.matrix, we could process
a larger volume of text in batches to incrementally construct our sparse
matrix. Equally, if we are building a classifier to assess as yet unseen
test cases we can independently hash the test data in the knowledge that
matching terms across training and test data will be hashed to the same
column index.</p>
</div>
<div id="training-xgboost" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Training XGBoost</h1>
<p>For this vignette we’ll train a classifier on 20,000 of the reviews
and validate its performance on the other 5,000. To enable access to all
of the XGBoost parameters we’ll also convert the document term matrix to
an xgb.DMatrix and create a watchlist to monitor both training and
validation set accuracy. The matrix remains sparse throughout the
process. Other R machine learning packages that accept sparse matrices
include glmnet and the support vector machine function of the e1071
package.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20000</span>); valid <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(imdb))[<span class="sc">-</span>train]</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>dtrain <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(d1[train,], <span class="at">label =</span> imdb<span class="sc">$</span>sentiment[train])</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>dvalid <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(d1[valid,], <span class="at">label =</span> imdb<span class="sc">$</span>sentiment[valid])</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>watch <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">train =</span> dtrain, <span class="at">valid =</span> dvalid)</span></code></pre></div>
<p>First we train a linear model, reducing the learning rate from the
default 0.3 to 0.02 and trying out 10 rounds of gradient descent. We
also specify classification error as our chosen evaluation metric for
the watchlist.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">xgb.train</span>(<span class="at">booster =</span> <span class="st">&quot;gblinear&quot;</span>, <span class="at">nrounds =</span> <span class="dv">10</span>, <span class="at">eta =</span> <span class="fl">0.02</span>,</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>                <span class="at">data =</span> dtrain, <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>                <span class="at">watchlist =</span> watch, <span class="at">eval_metric =</span> <span class="st">&quot;error&quot;</span>)</span></code></pre></div>
<pre><code>## [0]  train-error:0.070650    valid-error:0.145200
## [1]  train-error:0.060400    valid-error:0.137200
## [2]  train-error:0.053250    valid-error:0.130000
## [3]  train-error:0.047400    valid-error:0.125400
## [4]  train-error:0.041800    valid-error:0.122600
## [5]  train-error:0.036750    valid-error:0.121200
## [6]  train-error:0.032800    valid-error:0.119000
## [7]  train-error:0.029300    valid-error:0.117200
## [8]  train-error:0.026500    valid-error:0.115600
## [9]  train-error:0.024000    valid-error:0.114200</code></pre>
<p>That code chunk runs in just a few seconds and the validation error
is already down to a reasonable 12%. So FeatureHashing has kept our
memory requirement to about 50MB and XGBoost’s efficient approach to
logistic regression has ensured we can get rapid feedback on the
performance of the classifier.</p>
<p>With this particular dataset a tree-based classifier would take far
longer to train and tune than a linear model. Without attempting to run
it for a realistic number of rounds, the code below shows how easily we
can switch to the tree-based mode of XGBoost:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">xgb.train</span>(<span class="at">data =</span> dtrain, <span class="at">nrounds =</span> <span class="dv">10</span>, <span class="at">eta =</span> <span class="fl">0.02</span>, </span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>                <span class="at">max.depth =</span> <span class="dv">10</span>, <span class="at">colsample_bytree =</span> <span class="fl">0.1</span>,</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>                <span class="at">subsample =</span> <span class="fl">0.95</span>, <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>                <span class="at">watchlist =</span> watch, <span class="at">eval_metric =</span> <span class="st">&quot;error&quot;</span>)</span></code></pre></div>
<pre><code>## [0]  train-error:0.386600    valid-error:0.415600
## [1]  train-error:0.301200    valid-error:0.347200
## [2]  train-error:0.272550    valid-error:0.316200
## [3]  train-error:0.245400    valid-error:0.288000
## [4]  train-error:0.219400    valid-error:0.269800
## [5]  train-error:0.204200    valid-error:0.258400
## [6]  train-error:0.196550    valid-error:0.243200
## [7]  train-error:0.181250    valid-error:0.241800
## [8]  train-error:0.174900    valid-error:0.239400
## [9]  train-error:0.176650    valid-error:0.236600</code></pre>
<p>The above demonstration deliberately omits steps such as model tuning
but hopefully it illustrates a useful workflow that makes the most of
the FeatureHashing and XGBoost packages.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
